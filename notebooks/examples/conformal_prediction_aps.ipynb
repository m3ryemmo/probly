{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ada5e7c",
   "metadata": {},
   "source": [
    "# Adaptive Prediction Sets (APS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3ac81",
   "metadata": {},
   "source": [
    "### The Problem of Uncertainty in Machine Learning Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726859c4",
   "metadata": {},
   "source": [
    "Traditional ML models do not provide information about **uncertainty or reliabilty**. But that is **crucial** in criitical applications, that are in need of **guaranteed coverage**.\n",
    "\n",
    "An example for such application would be an autuonomous driving application. It is not enough to predcit \"It is a pedestrian\", the application has **to be confident** about something on the road being \"a pederian, cyclist or traffic sign\", to prevent serious concequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48e538",
   "metadata": {},
   "source": [
    "### Core Idea: Conformal Prediction\n",
    "\n",
    "While traditional ML models only provide point predictions, APS provide prediction sets. \n",
    "\n",
    "Traditional: Model predicts \"class 3\"\n",
    "\n",
    "APS: Model predicts \"{2, 3, 5}\" with 90% confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287dbbe",
   "metadata": {},
   "source": [
    "### Marginal Coverage\n",
    "\n",
    "Marginal coverage provides a statistical guarantee.\n",
    "\n",
    "$$\\mathbb{P}[Y_{n+1} \\in \\hat{C}_{n,\\alpha}(X_{n+1})] \\geq 1-\\alpha$$\n",
    "\n",
    "Where: \n",
    "- $\\mathbb{P}[\\cdot]$: Probability operator\n",
    "- $Y_{n+1}$: Unknown true label we want to predict\n",
    "- $\\hat{\\mathcal{C}}_{n,\\alpha}(\\cdot)$: prediction set function that maps features\n",
    "- $X_{n+1}$: Observed features of test point\n",
    "- $1-\\alpha$: Target probability of coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6cef74",
   "metadata": {},
   "source": [
    "Let's look at a simple coverage test, where $\\alpha=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c638ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images: 10\n",
      "Target coverage: 0.9 (90%)\n",
      "Actual coverage: 0.9 (90.0%)\n",
      "Coverage >= Target coverage: True\n",
      "Calculation: 9 / 10 = 0.9\n"
     ]
    }
   ],
   "source": [
    "test_images = 10\n",
    "alpha = 0.1\n",
    "target_coverage = 1 - alpha\n",
    "\n",
    "results = [1, 1, 1, 1, 1, 1, 1, 0, 1, 1]  # 9 out of 10 correct\n",
    "\n",
    "coverage = sum(results) / len(results)\n",
    "\n",
    "print(f\"Number of test images: {test_images}\")\n",
    "print(f\"Target coverage: {target_coverage} (90%)\")\n",
    "print(f\"Actual coverage: {coverage} ({coverage * 100}%)\")\n",
    "print(f\"Coverage >= Target coverage: {coverage >= target_coverage}\")\n",
    "print(f\"Calculation: {sum(results)} / {len(results)} = {coverage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931adb0",
   "metadata": {},
   "source": [
    "### The APS Algorithm\n",
    "\n",
    "Provides the Conformity Score:\n",
    "$$ E(x,y,u;\\hat{\\pi}) = \\min\\{\\tau \\in [0,1] : y \\in \\mathcal{S}(x,u;\\hat{\\pi},\\tau)\\} $$\n",
    "\n",
    "The conformity score measures the **minimum probability threshold** at which the true label would be included in the prediction set, quantifying how **\"surprised\"** the model is by the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69684190",
   "metadata": {},
   "source": [
    "Following: A Simple APS Conformity Score implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "219213e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def aps_conformity_score(self, probabilities: np.ndarray, true_labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"E(x,y,u;π̂) = min{τ ∈ [0,1] : y ∈ S(x,u;π̂,τ)}\n",
    "\n",
    "    Finds the minimum threshold τ at which the true class\n",
    "    is included in the prediction set.\n",
    "    \"\"\"\n",
    "    conformity_scores = []\n",
    "\n",
    "    for true_label, probs in zip(true_labels, probabilities, strict=False):\n",
    "        # descending\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        sorted_probs = probs[sorted_indices]\n",
    "\n",
    "        cumulative_probs = np.cumsum(sorted_probs)\n",
    "\n",
    "        true_class_pos = np.where(sorted_indices == true_label)[0][0]\n",
    "\n",
    "        # score is cumulative prob up to the true class\n",
    "        aps_score = cumulative_probs[true_class_pos]\n",
    "        conformity_scores.append(aps_score)\n",
    "\n",
    "    return np.array(conformity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0e5d5",
   "metadata": {},
   "source": [
    "Where inputs are probabilitites(model's predicted probabilities), true_labels.\n",
    "\n",
    "For each sample, sort classes by probability and find the **cumulative probability at whih the true class is included**.\n",
    "\n",
    "Low conformity score means the model is \"suprised\" by the true label.\n",
    "\n",
    "Mathemathical Interpretation: The minimum probability threshold τ where the true class y would be included in the prediction set S(x,τ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094cb10d",
   "metadata": {},
   "source": [
    "### APS implemented on Iris Dataset\n",
    "\n",
    "1. Whats is the Iris Dataset? \n",
    "The Iris Dataset is a classical benchmark dataset, whih contains 150 samples. It is an ideal dataset to test and teach such algoriths through examples.\n",
    "\n",
    "Heres an implementation of Iris and APS implemented:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b794be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class APSIrisImplementation:\n",
    "    \"\"\"Complete APS Implementation\"\"\"\n",
    "\n",
    "    def __init__(self, significance_level: float = 0.1):\n",
    "        self.significance_level = significance_level\n",
    "        self.model = None\n",
    "        self.quantile = None\n",
    "        self.calibration_scores = None\n",
    "\n",
    "    def load_prepare_data(self) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load and split the Iris dataset\"\"\"\n",
    "        iris = load_iris()\n",
    "        X, y = iris.data, iris.target\n",
    "        feature_names = iris.feature_names\n",
    "        target_names = iris.target_names\n",
    "\n",
    "        # split inot train, calib, and test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        X_train, X_calib, y_train, y_calib = train_test_split(\n",
    "            X_temp,\n",
    "            y_temp,\n",
    "            test_size=0.25,\n",
    "            random_state=42,\n",
    "            stratify=y_temp,\n",
    "        )\n",
    "\n",
    "        return X_train, X_calib, X_test, y_train, y_calib, y_test, feature_names, target_names\n",
    "\n",
    "    def train_model(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"Train base classifier\"\"\"\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        return self.model\n",
    "\n",
    "    def aps_conformity_score(self, probabilities: np.ndarray, true_labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"E(x,y,u;π̂) = min{τ ∈ [0,1] : y ∈ S(x,u;π̂,τ)}\n",
    "\n",
    "        Finds the minimum threshold τ at which the true class\n",
    "        is included in the prediction set.\n",
    "        \"\"\"\n",
    "        conformity_scores = []\n",
    "\n",
    "        for true_label, probs in zip(true_labels, probabilities, strict=False):\n",
    "            # descending\n",
    "            sorted_indices = np.argsort(probs)[::-1]\n",
    "            sorted_probs = probs[sorted_indices]\n",
    "\n",
    "            cumulative_probs = np.cumsum(sorted_probs)\n",
    "\n",
    "            true_class_pos = np.where(sorted_indices == true_label)[0][0]\n",
    "\n",
    "            # score is cumulative prob up to the true class\n",
    "            aps_score = cumulative_probs[true_class_pos]\n",
    "            conformity_scores.append(aps_score)\n",
    "\n",
    "        return np.array(conformity_scores)\n",
    "\n",
    "    def calculate_prediction_sets(self, probabilities: np.ndarray, tau: float) -> list[list[int]]:\n",
    "        \"\"\"Calculates prediction sets for for given threshold tau\"\"\"\n",
    "        prediction_sets = []\n",
    "\n",
    "        for probs in probabilities:\n",
    "            sorted_indices = np.argsort(probs)[::-1]\n",
    "            sorted_probs = probs[sorted_indices]\n",
    "            cumulative_probs = np.cumsum(sorted_probs)\n",
    "\n",
    "            prediction_set = []\n",
    "            for i, (class_idx, cum_prob) in enumerate(zip(sorted_indices, cumulative_probs, strict=False)):\n",
    "                if cum_prob <= tau or i == 0:\n",
    "                    prediction_set.append(class_idx)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            prediction_sets.append(prediction_set)\n",
    "\n",
    "        return prediction_sets\n",
    "\n",
    "    def fit_aps(self, X_train: np.ndarray, y_train: np.ndarray, X_calib: np.ndarray, y_calib: np.ndarray):\n",
    "        \"\"\"Trains model and calibrates APS\"\"\"\n",
    "        # train model\n",
    "        self.train_model(X_train, y_train)\n",
    "        # calculate confirmity scoer on data\n",
    "        calib_probs = self.model.predict_proba(X_calib)\n",
    "        self.calibration_scores = self.aps_conformity_score(calib_probs, y_calib)\n",
    "\n",
    "        # calculate quantile for set\n",
    "        self.quantile = np.quantile(self.calibration_scores, self.significance_level)\n",
    "\n",
    "        print(\"APS Calibration Complete:\")\n",
    "        print(f\"  Significance level (α): {self.significance_level}\")\n",
    "        print(f\"  Calibration quantile: {self.quantile:.3f}\")\n",
    "        print(f\"  Target coverage: {1 - self.significance_level:.1%}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_with_aps(self, X: np.ndarray) -> tuple[np.ndarray, list[list[int]], np.ndarray]:\n",
    "        \"\"\"Makes predictions with APS sets\"\"\"\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        point_predictions = self.model.predict(X)\n",
    "        prediction_sets = self.calculate_prediction_sets(probabilities, self.quantile)\n",
    "        aps_scores = self.aps_conformity_score(probabilities, point_predictions)\n",
    "\n",
    "        return point_predictions, prediction_sets, aps_scores\n",
    "\n",
    "    def evaluate_aps(self, X_test: np.ndarray, y_test: np.ndarray) -> dict[str, float]:\n",
    "        \"\"\"Evaluates APS performance\"\"\"\n",
    "        point_preds, prediction_sets, aps_scores = self.predict_with_aps(X_test)\n",
    "\n",
    "        # calculate metrics\n",
    "        coverage = np.mean(\n",
    "            [true_label in pred_set for true_label, pred_set in zip(y_test, prediction_sets, strict=False)],\n",
    "        )\n",
    "        avg_set_size = np.mean([len(pred_set) for pred_set in prediction_sets])\n",
    "        accuracy = accuracy_score(y_test, point_preds)\n",
    "\n",
    "        # calculate set size distribution\n",
    "        set_sizes = [len(pred_set) for pred_set in prediction_sets]\n",
    "        single_pred_rate = np.mean(np.array(set_sizes) == 1)\n",
    "        multi_pred_rate = np.mean(np.array(set_sizes) > 1)\n",
    "\n",
    "        return {\n",
    "            \"coverage\": coverage,\n",
    "            \"average_set_size\": avg_set_size,\n",
    "            \"point_accuracy\": accuracy,\n",
    "            \"empirical_alpha\": 1 - coverage,\n",
    "            \"single_prediction_rate\": single_pred_rate,\n",
    "            \"multi_prediction_rate\": multi_pred_rate,\n",
    "            \"aps_scores\": aps_scores,\n",
    "            \"set_sizes\": set_sizes,\n",
    "        }\n",
    "\n",
    "\n",
    "def create_visualisation(\n",
    "    aps_system: APSIrisImplementation,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    target_names: list[str],\n",
    "    results: dict[str, Any],\n",
    "):\n",
    "    \"\"\"Comprehensive visualisation of APS results\"\"\"\n",
    "    # get prediction and detail\n",
    "    point_preds, predition_sets, aps_scores = aps_system.predict_with_aps(X_test)\n",
    "    probabilities = aps_system.model.predict_proba(X_test)\n",
    "\n",
    "    # create fig\n",
    "    fig = plt.figure(figuresize=(20, 16))\n",
    "\n",
    "    # create aps score distribution\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    plt.hist(results[\"aps_scores\"], bins=20, alpha=0.7, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.axvline(\n",
    "        x=aps_system.quantile,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Calibration Quantile (τ={aps_system.quantile:.3f})\",\n",
    "    )\n",
    "    plt.xlabel(\"APS Conformity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title('Distribution of APS Conformity Scores\\n(Lower = More \"Surprised\")')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # prediction Set Sizes\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    set_size_counts = pd.Series(results[\"set_sizes\"]).value_counts().sort_index()\n",
    "    plt.bar(set_size_counts.index, set_size_counts.values, color=\"lightgreen\", alpha=0.7, edgecolor=\"black\")\n",
    "    plt.xlabel(\"Prediction Set Size\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.title(\"Distribution of Prediction Set Sizes\")\n",
    "    for i, v in enumerate(set_size_counts.values):\n",
    "        plt.text(set_size_counts.index[i], v + 0.5, str(v), ha=\"center\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # coverage by Class\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    coverage_by_class = []\n",
    "    for class_label in range(len(target_names)):\n",
    "        class_mask = y_test == class_label\n",
    "        class_coverage = np.mean(\n",
    "            [\n",
    "                true_label in pred_set\n",
    "                for true_label, pred_set in zip(y_test[class_mask], np.array(prediction_sets)[class_mask], strict=False)\n",
    "            ],\n",
    "        )\n",
    "        coverage_by_class.append(class_coverage)\n",
    "\n",
    "    bars = plt.bar(range(len(target_names)), coverage_by_class, color=[\"lightcoral\", \"lightgreen\", \"lightblue\"])\n",
    "    plt.axhline(\n",
    "        y=1 - aps_system.significance_level,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Target Coverage ({1 - aps_system.significance_level:.1%})\",\n",
    "    )\n",
    "    plt.xlabel(\"Iris Species\")\n",
    "    plt.ylabel(\"Coverage Rate\")\n",
    "    plt.title(\"Coverage Rate by Class\")\n",
    "    plt.xticks(range(len(target_names)), target_names, rotation=45)\n",
    "    plt.legend()\n",
    "    for i, v in enumerate(coverage_by_class):\n",
    "        plt.text(i, v + 0.02, f\"{v:.3f}\", ha=\"center\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # feature Space Visualization (First 2 features)\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    colors = [\"red\", \"green\", \"blue\"]\n",
    "    set_size_markers = [\"o\", \"s\", \"D\"]  # circle, square, diamond\n",
    "\n",
    "    for i, (true_label, pred_set, aps_score) in enumerate(zip(y_test, prediction_sets, aps_scores, strict=False)):\n",
    "        marker_size = 100 if len(pred_set) == 1 else 150\n",
    "        marker_alpha = 0.7 if len(pred_set) == 1 else 0.9\n",
    "\n",
    "        if true_label in pred_set:\n",
    "            # Correct prediction - green border\n",
    "            facecolor = colors[true_label]\n",
    "            edgecolor = \"green\"\n",
    "            linewidth = 2\n",
    "        else:\n",
    "            # Incorrect prediction - red border\n",
    "            facecolor = colors[true_label]\n",
    "            edgecolor = \"red\"\n",
    "            linewidth = 3\n",
    "\n",
    "        plt.scatter(\n",
    "            X_test[i, 0],\n",
    "            X_test[i, 1],\n",
    "            c=facecolor,\n",
    "            marker=set_size_markers[len(pred_set) - 1],\n",
    "            s=marker_size,\n",
    "            alpha=marker_alpha,\n",
    "            edgecolor=edgecolor,\n",
    "            linewidth=linewidth,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.title(\"Feature Space with APS Prediction Sets\\n(Marker: ○=1 class, □=2 classes, ◊=3 classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e945e2c",
   "metadata": {},
   "source": [
    "Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad70fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple, List, Dict, Any\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run APS on Iris dataset\"\"\"\n",
    "    print(\"=== APS Implementation on Iris Dataset ===\\n\")\n",
    "\n",
    "    #initialize APS system\n",
    "    aps_system = APSIrisImplementation(significance_level=0.1)\n",
    "\n",
    "    #load data\n",
    "    X_train, X_calib, X_test, y_train, y_calib, y_test, feature_names, target_names = aps_system.load_prepare_data()\n",
    "\n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"  Features: {feature_names}\")\n",
    "    print(f\"  Classes: {list(target_names)}\")\n",
    "    print(f\"  Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"  Calibration samples: {X_calib.shape[0]}\")\n",
    "    print(f\"  Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "    #train and calibrate APS\n",
    "    aps_system.fit_aps(X_train, y_train, X_calib, y_calib)\n",
    "\n",
    "    #evaluate APS\n",
    "    results = aps_system.evaluate_aps(X_test, y_test)\n",
    "\n",
    "    print(\"\\nAPS Evaluation Results:\")\n",
    "    print(f\"  Coverage rate: {results['coverage']:.3f}\")\n",
    "    print(f\"  Average set size: {results['average_set_size']:.2f}\")\n",
    "    print(f\"  Point prediction accuracy: {results['point_accuracy']:.3f}\")\n",
    "\n",
    "    #create visualization\n",
    "    create_visualisation(aps_system, X_test, y_test, feature_names, target_names, results)\n",
    "\n",
    "    return aps_system, results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aps_system, results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
